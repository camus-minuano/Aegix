import os
import csv
import random
from tabulate import tabulate
from typing import List, Tuple, Dict, Any

import config # For DATA_DIR, ATTACK_FILES, etc.

def load_single_csv(filepath: str, seen_texts: set) -> List[str]:
    """Loads texts from a single CSV file, ensuring uniqueness across all files."""
    rows = []
    if not os.path.isfile(filepath):
        print(f"âš ï¸  æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}ï¼Œå·²è·³è¿‡ã€‚")
        return rows
    try:
        with open(filepath, newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            next(reader, None) # Skip header row
            for row in reader:
                if len(row) >= 2: # Assuming text is in the second column (index 1)
                    text = row[1].strip()
                    if text and text not in seen_texts:
                        rows.append(text)
                        seen_texts.add(text)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶ {os.path.basename(filepath)} æ—¶å‡ºé”™: {e}")
    return rows

def load_and_sample_datasets(
    data_dir: str = config.DATA_DIR,
    attack_files: List[str] = config.ATTACK_FILES,
    harmless_files: List[str] = config.HARMLESS_FILES,
    sample_size: int = config.DEFAULT_SAMPLE_SIZE,
    base_test_cases: List[Tuple[str, bool]] = None
) -> List[Tuple[str, bool]]:
    """
    Loads data from specified CSV files, performs stratified sampling,
    and combines with base test cases.
    Returns a list of (text, should_block_label).
    """
    if base_test_cases is None:
        test_cases: List[Tuple[str, bool]] = []
    else:
        test_cases = list(base_test_cases) # Make a mutable copy

    file_specs: List[Dict[str, Any]] = []
    seen_texts_global: set = {text for text, _ in test_cases} # Initialize with texts from base_test_cases

    # Load attack samples
    for fname in attack_files:
        fpath = os.path.join(data_dir, fname)
        rows = load_single_csv(fpath, seen_texts_global)
        if rows:
            file_specs.append({"fname": fname, "label": True, "rows": rows, "type": "Attack"})

    # Load harmless samples
    for fname in harmless_files:
        fpath = os.path.join(data_dir, fname)
        rows = load_single_csv(fpath, seen_texts_global)
        if rows:
            file_specs.append({"fname": fname, "label": False, "rows": rows, "type": "Benign"})

    if not file_specs:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å¤–éƒ¨æ ·æœ¬æ–‡ä»¶ï¼Œä»…ä½¿ç”¨å†…ç½®æµ‹è¯•ç”¨ä¾‹ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚")
        return test_cases

    # Calculate quotas for sampling
    num_files = len(file_specs)
    if num_files == 0: # Should be caught by above, but defensive
        return test_cases

    # Distribute sample_size proportionally or equally
    # This logic aims for balanced sampling if sample_size is the target for *new* samples
    
    # If sample_size is total including base cases, adjust:
    # current_external_samples = sample_size - len(base_test_cases)
    # target_external_samples = max(0, current_external_samples)
    
    target_external_samples = sample_size # Assume sample_size is for external files

    if target_external_samples <= 0:
        print("æŠ½æ ·æ•°é‡é…ç½®ä¸º0æˆ–è´Ÿæ•°ï¼Œæˆ–æ‰€æœ‰æ ·æœ¬å·²ç”±å†…ç½®ç”¨ä¾‹æä¾›ã€‚")
        return test_cases
        
    base_quota = target_external_samples // num_files
    extra_quota = target_external_samples % num_files

    for idx, fs in enumerate(file_specs):
        fs["quota"] = base_quota + (1 if idx < extra_quota else 0)

    # Adjust quotas if a file has fewer samples than its quota
    deficit = 0
    for fs in file_specs:
        if len(fs["rows"]) < fs["quota"]:
            deficit += fs["quota"] - len(fs["rows"])
            fs["quota"] = len(fs["rows"])

    # Redistribute deficit to files that have more samples
    # This loop ensures we try to meet target_external_samples if possible
    while deficit > 0:
        updated_in_pass = False
        eligible_files_for_extra = [fs for fs in file_specs if len(fs["rows"]) > fs["quota"]]
        if not eligible_files_for_extra:
            break # No files can take more samples

        # Distribute deficit among eligible files
        # A simple way is to give one to each until deficit is consumed or files are maxed out
        for fs in eligible_files_for_extra:
            if deficit == 0: break
            fs["quota"] += 1
            deficit -= 1
            updated_in_pass = True
        if not updated_in_pass: # No changes made, avoid infinite loop
            break


    print("\n--- æ•°æ®é›†æŠ½æ ·è®¡åˆ’ ---")
    total_sampled_count = 0
    for fs in file_specs:
        num_to_sample = min(fs["quota"], len(fs["rows"])) # Ensure not to sample more than available
        if num_to_sample > 0:
            picked_samples = random.sample(fs["rows"], num_to_sample)
            test_cases.extend([(text, fs["label"]) for text in picked_samples])
            total_sampled_count += num_to_sample
        print(f"  - æ–‡ä»¶: {fs['fname']:<20} ({fs['type']}) | è®¡åˆ’æŠ½æ ·: {fs['quota']} | å®é™…å¯ç”¨: {len(fs['rows'])} | æœ€ç»ˆæŠ½æ ·: {num_to_sample}")
    
    print(f"æ€»å…±ä»å¤–éƒ¨æ–‡ä»¶æŠ½æ ·: {total_sampled_count} æ¡")
    print(f"æµ‹è¯•ç”¨ä¾‹æ€»æ•° (åŒ…æ‹¬å†…ç½®): {len(test_cases)} æ¡")
    print("--- æŠ½æ ·å®Œæˆ ---\n")
    
    random.shuffle(test_cases) # Shuffle all test cases together
    return test_cases


def live_update_table(results_for_current_sample: List[Dict[str, Any]]):
    """
    å®æ—¶æ›´æ–°è¡¨æ ¼ä»¥æ˜¾ç¤ºå½“å‰æ ·æœ¬åœ¨ä¸åŒæ£€æµ‹è·¯å¾„ä¸‹çš„ç»“æœã€‚
    :param results_for_current_sample: A list of result dicts, one for each path for the *current* sample.
                                       e.g., [regex_result, model_result, full_result]
    """
    headers = ["æ£€æµ‹è·¯å¾„", "æ‹¦æˆªçŠ¶æ€", "è€—æ—¶(ms)", "è§¦å‘è§„åˆ™/åŸå›  (éƒ¨åˆ†)"]
    rows = []

    for res in results_for_current_sample:
        status_emoji = "ğŸ”´ æ‹¦æˆª" if res.get("is_blocked", False) else "ğŸŸ¢ æ”¾è¡Œ"
        time_ms_str = f"{res.get('time_cost', 0)*1000:.1f}"
        
        rules_or_reason = res.get("triggered_rules", [])
        if isinstance(rules_or_reason, list):
            display_rules = "\n".join(rules_or_reason[:2]) if rules_or_reason else "æ— "
        elif isinstance(rules_or_reason, str): # For single string reasons
            display_rules = rules_or_reason[:100] + "..." if len(rules_or_reason) > 100 else rules_or_reason
        else:
            display_rules = "ä¿¡æ¯ä¸å¯ç”¨"

        rows.append([
            res.get("path", "æœªçŸ¥è·¯å¾„"),
            status_emoji,
            time_ms_str,
            display_rules
        ])

    # Clear previous output (works better in some terminals)
    # os.system('cls' if os.name == 'nt' else 'clear') # This can be disruptive, use with caution or avoid.
    
    print("\n" + "~" * 60)
    print("å½“å‰æ ·æœ¬æ£€æµ‹ç»“æœ:")
    print(tabulate(rows, headers=headers, tablefmt="grid", maxcolwidths=[None, None, None, 40])) # Limit rule column width
    print("~" * 60 + "\n")